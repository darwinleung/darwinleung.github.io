<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Processing on DL</title>
    <link>https://darwinleung.github.io/categories/data-processing/</link>
    <description>Recent content in Data Processing on DL</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 12 May 2018 12:00:00 -0400</lastBuildDate>
    
	<atom:link href="https://darwinleung.github.io/categories/data-processing/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How does imputation work</title>
      <link>https://darwinleung.github.io/posts/2018-05-12-imputation-sklearn/</link>
      <pubDate>Sat, 12 May 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-05-12-imputation-sklearn/</guid>
      <description>How does imputation work? Imputation is a pre-processing technique to handle missing data before fitting the data into model. One of the simplest implementation is to estimate the missing values using the mean/median or the most frequent value of the row/ column where the missing values are located. There are more advanced ways to impute data like regressions, multiple imputation etc.
Imputer in sklearn The Imputer function in the sklearn library is commonly used in various tutorials.</description>
    </item>
    
    <item>
      <title>Extracting features from text - BoW, tfidf</title>
      <link>https://darwinleung.github.io/posts/2018-05-09-extracting-features-from-text-bow-tfidf/</link>
      <pubDate>Wed, 09 May 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-05-09-extracting-features-from-text-bow-tfidf/</guid>
      <description>Extracting features from text Bag-of-words TF-IDF  Term-frequency-inverse document frequency Numeriacal statistics to reflect how important a word is to a document Used as weighting factor in text mining, it can also be used to generate word vector/ matrix representation Increase proportionally to the number of times a word appears in the document offset by the frequency of the word in the corpus  $$ tfidf(t,d,D) = tf(t,d) * idf(t,D) $$</description>
    </item>
    
  </channel>
</rss>
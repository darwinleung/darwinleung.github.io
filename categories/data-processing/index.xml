<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Processing on DL</title>
    <link>https://darwinleung.github.io/categories/data-processing/</link>
    <description>Recent content in Data Processing on DL</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 01 Sep 2018 12:00:00 -0400</lastBuildDate>
    
	<atom:link href="https://darwinleung.github.io/categories/data-processing/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Modelling Snowplow event data notes</title>
      <link>https://darwinleung.github.io/posts/2018-09-01-modelling-snowplow-event-data-notes/</link>
      <pubDate>Sat, 01 Sep 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-09-01-modelling-snowplow-event-data-notes/</guid>
      <description>Modelling Snowplow event data notes Goals:
 Modelling pageviews level metric (timeonpage, scroll depth) Modelling sessions level metric (session count, session duration)  Note: All queries in this doc are NOT tested or validated, they are based on assumptions and my guess only for high level understanding only. Please always consult Snowflake and make modification accordingly before using them for production.
Validating readily available metrics First, we would like to see if the pageviews count collected from Snowplow match roughly with the data we collected from GA:</description>
    </item>
    
    <item>
      <title>How does imputation work</title>
      <link>https://darwinleung.github.io/posts/2018-05-12-imputation-sklearn/</link>
      <pubDate>Sat, 12 May 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-05-12-imputation-sklearn/</guid>
      <description>How does imputation work? Imputation is a pre-processing technique to handle missing data before fitting the data into model. One of the simplest implementation is to estimate the missing values using the mean/median or the most frequent value of the row/ column where the missing values are located. There are more advanced ways to impute data like regressions, multiple imputation etc.
Imputer in sklearn The Imputer function in the sklearn library is commonly used in various tutorials.</description>
    </item>
    
    <item>
      <title>Extracting features from text - BoW, tfidf</title>
      <link>https://darwinleung.github.io/posts/2018-05-09-extracting-features-from-text-bow-tfidf/</link>
      <pubDate>Wed, 09 May 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-05-09-extracting-features-from-text-bow-tfidf/</guid>
      <description>Most of the machine learning algorithms work with numerical features only, they cannot work with raw text directly. Hence, we need to extract/generate numerical representation of textual data. One of the simplest way is to use the occurance or frequency of words.
Bag-of-words (BoW)  Representation of text that describes the occurence of words within a document
 Step:
 Find all unique words in a document, used as column (aka token) Define the measure method (1&amp;frasl;0 for presence or absence of words, count, tfidf) Calculate the measurement for each sentence, each row correspond to each sentence (aka tokenization)   Example:</description>
    </item>
    
  </channel>
</rss>
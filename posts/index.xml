<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on DL</title>
    <link>https://darwinleung.github.io/posts/</link>
    <description>Recent content in Posts on DL</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 01 Sep 2018 12:00:00 -0400</lastBuildDate>
    
	<atom:link href="https://darwinleung.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Modelling Snowplow event data notes</title>
      <link>https://darwinleung.github.io/posts/2018-09-01-modelling-snowplow-event-data-notes/</link>
      <pubDate>Sat, 01 Sep 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-09-01-modelling-snowplow-event-data-notes/</guid>
      <description>Modelling Snowplow event data notes Goals:
 Modelling pageviews level metric (timeonpage, scroll depth) Modelling sessions level metric (session count, session duration)  Note: All queries in this doc are NOT tested or validated, they are based on assumptions and my guess only for high level understanding only. Please always consult Snowflake and make modification accordingly before using them for production.
Validating readily available metrics First, we would like to see if the pageviews count collected from Snowplow match roughly with the data we collected from GA:</description>
    </item>
    
    <item>
      <title>BigQuery notes</title>
      <link>https://darwinleung.github.io/posts/2018-08-01-bigquery-notes/</link>
      <pubDate>Wed, 01 Aug 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-08-01-bigquery-notes/</guid>
      <description>There is a new project at work, we store snowplow events data on Google BigQuery (vs on AWS S3 conventionally). The query language is very similar with SQL however, there is some differences in term of best practice and cost. Here is some notes.
BigQuery vs MySQL BigQuery: fully-managed cloud-based enterprise data warehouse. Good for OLAP task (i.e. low volume of transactions, complex query with aggregation). Distributed storage and parallel computation for query big scale data very fast.</description>
    </item>
    
    <item>
      <title>Personalized recommendation system for articles - Content-based</title>
      <link>https://darwinleung.github.io/posts/2018-07-02-personalized-recommendation-system-content-based/</link>
      <pubDate>Mon, 02 Jul 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-07-02-personalized-recommendation-system-content-based/</guid>
      <description>Personalized recommendation system for articles - Content-based We use the same example from CF post, let&amp;rsquo;s say we have the same 6 articles and 5 visitors and their view history data. Also, we have data about the context of each articles, like title, author, categories, length, no. of picture, and the words from the articles. We might also have data from the user side like their demographic, gender, ages, country etc.</description>
    </item>
    
    <item>
      <title>Personalized recommendation system for articles - Collaborative Filtering</title>
      <link>https://darwinleung.github.io/posts/2018-07-01-personalized-recommendation-system-collaborative-filtering/</link>
      <pubDate>Sun, 01 Jul 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-07-01-personalized-recommendation-system-collaborative-filtering/</guid>
      <description>Personalized recommendation system for articles - Collaborative Filtering From Andrew Ng&amp;rsquo;s course, I learned there are 2 popular ways to built recommendation system: Collaborative Filtering and Content-based. Recently, I have a chance to research and implement them at work. Here is a Proof-of-Concept example.
Recommendation system is about recommending items to users. Let&amp;rsquo;s say we only have:
 Items: 6 articles: [&amp;lsquo;dog1&amp;rsquo;, &amp;lsquo;dog2&amp;rsquo;, &amp;lsquo;cat&amp;rsquo;, &amp;lsquo;food1&amp;rsquo;, &amp;lsquo;food2&amp;rsquo;, &amp;lsquo;food3&amp;rsquo;]. (Note: we label them just for easier explanation, it is not necessary to have any context about the items, they can be random ids) Users: 5 visitors [&amp;lsquo;user_0&amp;rsquo;,&amp;lsquo;user_1&amp;rsquo;,&amp;lsquo;user_2&amp;rsquo;, &amp;lsquo;user_3&amp;rsquo;, &amp;lsquo;user_4&amp;rsquo;, &amp;lsquo;user_5&amp;rsquo;]  Collaborative Filtering  Collaborative filteringÂ approaches build a model from a user&amp;rsquo;s past behaviour (items previously purchased or selected and/or numerical ratings given to those items) as well as similar decisions made by other users.</description>
    </item>
    
    <item>
      <title>How does imputation work</title>
      <link>https://darwinleung.github.io/posts/2018-05-12-imputation-sklearn/</link>
      <pubDate>Sat, 12 May 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-05-12-imputation-sklearn/</guid>
      <description>How does imputation work? Imputation is a pre-processing technique to handle missing data before fitting the data into model. One of the simplest implementation is to estimate the missing values using the mean/median or the most frequent value of the row/ column where the missing values are located. There are more advanced ways to impute data like regressions, multiple imputation etc.
Imputer in sklearn The Imputer function in the sklearn library is commonly used in various tutorials.</description>
    </item>
    
    <item>
      <title>Extracting features from text - BoW, tfidf</title>
      <link>https://darwinleung.github.io/posts/2018-05-09-extracting-features-from-text-bow-tfidf/</link>
      <pubDate>Wed, 09 May 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-05-09-extracting-features-from-text-bow-tfidf/</guid>
      <description>Most of the machine learning algorithms work with numerical features only, they cannot work with raw text directly. Hence, we need to extract/generate numerical representation of textual data. One of the simplest way is to use the occurance or frequency of words.
Bag-of-words (BoW)  Representation of text that describes the occurence of words within a document
 Step:
 Find all unique words in a document, used as column (aka token) Define the measure method (1&amp;frasl;0 for presence or absence of words, count, tfidf) Calculate the measurement for each sentence, each row correspond to each sentence (aka tokenization)   Example:</description>
    </item>
    
    <item>
      <title>Writing Math in Hugo</title>
      <link>https://darwinleung.github.io/posts/2018-04-18-writing-math-in-hugo/</link>
      <pubDate>Wed, 18 Apr 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-04-18-writing-math-in-hugo/</guid>
      <description>In this blog, I am going to write a lot of math. I used Typora as my markdown editor and it supports MathJax out of the box. I just need to wrap my math blocks with $$ (following this reference). However, I will need to do a quick step to make Hugo work with MathJax as discribed in the Hugo documentation. I copied add-mathjax-to-page.html and pasted in the beginning of post_footer.</description>
    </item>
    
    <item>
      <title>MongoDB for Developer M101P notes</title>
      <link>https://darwinleung.github.io/posts/2018-04-10-mongodb-notes/</link>
      <pubDate>Tue, 10 Apr 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-04-10-mongodb-notes/</guid>
      <description>insertMany(), default ordered: true, once it encounters as error, insert stop. If ordered set to false, it will insert the rest of documents despite any errors.
 ObjectId: 12-Byte Hex string
  | _ _ _ _ | _ _ _ | _ _ | _ _ _ | | &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; | &amp;mdash;&amp;mdash;&amp;ndash; | &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- | &amp;mdash;&amp;mdash;- | | Date (time stamp) | Mac addr | PID (process ID) | Counter |</description>
    </item>
    
    <item>
      <title>SQL optimization with indexing - part 3 - multi-column index</title>
      <link>https://darwinleung.github.io/posts/2018-04-04-sql-optimization-with-indexing-part-2/</link>
      <pubDate>Wed, 04 Apr 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-04-04-sql-optimization-with-indexing-part-2/</guid>
      <description>MySQL can only use one index for each SELECT statement, it is very common to perform operations on multiple columns like:
SELECT gaAppIds, dimension_date, dimension_country, dimension_source, dimension_medium, SUM(metric_sessions) FROM country_analytics GROUP BY gaAppIds , dimension_date , dimension_source , dimension_medium , dimension_country; By using a single-column index, we cannot take the full advantage for this query. We can add index for multiple columns (up to 16 columns). In the example above, we can add the following index with the 5 columns in the GROUP BY :</description>
    </item>
    
    <item>
      <title>SQL optimization with indexing - part 2 - index types in MySQL</title>
      <link>https://darwinleung.github.io/posts/2018-04-03-sql-optimization-with-indexing-part-2/</link>
      <pubDate>Tue, 03 Apr 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-04-03-sql-optimization-with-indexing-part-2/</guid>
      <description>There are 2 main data structures for index in MySQL, they are B-tree and hash table. They are also the main index structures for other RMDBS.
B-tree B-tree is a self-balancing sorted data structure similar to a binary search tree. The only difference is that a node in B-tree can have more than 2 children. For me, it looks like a binary tree with a range of numbers in each node (vs only 1 number in a node).</description>
    </item>
    
    <item>
      <title>SQL optimization with indexing - part 1</title>
      <link>https://darwinleung.github.io/posts/2018-04-02-sql-optimization-with-indexing-part-1/</link>
      <pubDate>Mon, 02 Apr 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-04-02-sql-optimization-with-indexing-part-1/</guid>
      <description>Knowing how to write a query is not enough, when our tables grow larger it is common to run into query performance issues. If we know in advance how we are querying the table, one very handful technique to &amp;ldquo;magically&amp;rdquo; speed up the query is to add appropriate indexes in tables.
What is index? From MySQL documentation:
 Indexes are used to find rows with specific column values quickly. Without an index, MySQL must begin with the first row and then read through the entire table to find the relevant rows.</description>
    </item>
    
    <item>
      <title>Handy SQL Queries</title>
      <link>https://darwinleung.github.io/posts/2018-04-01-handy-sql-queries/</link>
      <pubDate>Sun, 01 Apr 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-04-01-handy-sql-queries/</guid>
      <description>This is a list of handy SQL queries I use, I will keep it updated occasionally.
Getting table sizes in MySQL This query lists the size of every table in every database in an MySQL instance, size sorted desc in MB:
SELECT table_schema as `Database`, table_name AS `Table`, round(((data_length + index_length) / 1024 / 1024), 2) `Size in MB` FROM information_schema.TABLES ORDER BY (data_length + index_length) DESC;	 ref: SO</description>
    </item>
    
    <item>
      <title>Running python 2 and 3 using conda</title>
      <link>https://darwinleung.github.io/posts/2018-03-30-running-python-2-and-3-using-conda/</link>
      <pubDate>Fri, 30 Mar 2018 12:37:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-03-30-running-python-2-and-3-using-conda/</guid>
      <description>I am taking the M101P course to brush up my Mongo skill, the course is kind of dated, it uses Python 2. However, I use Python 3 on my machine by default. I need a quick and easy way to run python 2 scripts without changing my default settings/path. I found conda offer a great out-of-box solution by using environment.
 A conda environment is a directory that contains a specific collection of conda packages that you have installed.</description>
    </item>
    
    <item>
      <title>Limbo</title>
      <link>https://darwinleung.github.io/posts/2018-03-26-limbo/</link>
      <pubDate>Mon, 26 Mar 2018 22:40:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-03-26-limbo/</guid>
      <description>We finished a indie game called &amp;ldquo;Limbo&amp;rdquo; over the weekend. Personally I like it very much, It is a short, simple, monochrome, 2D puzzle solving game. Although there is no exciting story or flashy graphic, the music and animation is outstanding, puzzles are well designed, it kept us engaged for hours.
I bought this game long time ago, never get time to try it until this Saturday I was downloading another large game, I decided to give it a try while we wait.</description>
    </item>
    
    <item>
      <title>Setting up this blog with Hugo and Hyde-hyde</title>
      <link>https://darwinleung.github.io/posts/2018-03-25-setting-up-this-blog/</link>
      <pubDate>Sun, 25 Mar 2018 14:47:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-03-25-setting-up-this-blog/</guid>
      <description>Hugo First, I follow the quick start guide to install and set up a hugo server locally.
Adding content Use hugo new posts/my-first-post.md or simply add a markdown file under the content\posts folder. Make sure the markdown files include the front matter, in a regular post page, it looks something like:
title: &amp;quot;Setting up this blog&amp;quot; date: 2018-03-25T14:47:00-04:00 draft: false tags: [ &amp;quot;hugo&amp;quot; , &amp;quot;git&amp;quot; ] categories: [ &amp;quot;programming&amp;quot; ]  For a static page like &amp;ldquo;About&amp;rdquo; or &amp;ldquo;Contact&amp;rdquo;, I only use title and draft.</description>
    </item>
    
    <item>
      <title>First Post - 2018 Goals</title>
      <link>https://darwinleung.github.io/posts/2018-03-01-first-post/</link>
      <pubDate>Thu, 01 Mar 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-03-01-first-post/</guid>
      <description>This is my first personal blog and this is my first blog post, building a blog is one of the goals I set in the beginning of 2018. 2018 would be my year of learning, I will spend my spare time on learning for both professionally and personally. I am lucky that there are many area overlapping.
There are 3 main area I want to learn:
Machine Learning As an aspiring data scientist, I need to build a strong foundation on statistical and machine learning and gain exposure in deep learning.</description>
    </item>
    
  </channel>
</rss>
<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science on DL</title>
    <link>https://darwinleung.github.io/categories/data-science/</link>
    <description>Recent content in Data Science on DL</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Jul 2018 12:00:00 -0400</lastBuildDate>
    
	<atom:link href="https://darwinleung.github.io/categories/data-science/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Personalized recommendation system for articles - Content-based</title>
      <link>https://darwinleung.github.io/posts/2018-07-02-personalized-recommendation-system-content-based/</link>
      <pubDate>Mon, 02 Jul 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-07-02-personalized-recommendation-system-content-based/</guid>
      <description>Personalized recommendation system for articles - Content-based We use the same example from CF post, let&amp;rsquo;s say we have the same 6 articles and 5 visitors and their view history data. Also, we have data about the context of each articles, like title, author, categories, length, no. of picture, and the words from the articles. We might also have data from the user side like their demographic, gender, ages, country etc.</description>
    </item>
    
    <item>
      <title>Personalized recommendation system for articles - Collaborative Filtering</title>
      <link>https://darwinleung.github.io/posts/2018-07-01-personalized-recommendation-system-collaborative-filtering/</link>
      <pubDate>Sun, 01 Jul 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-07-01-personalized-recommendation-system-collaborative-filtering/</guid>
      <description>Personalized recommendation system for articles - Collaborative Filtering From Andrew Ng&amp;rsquo;s course, I learned there are 2 popular ways to built recommendation system: Collaborative Filtering and Content-based. Recently, I have a chance to research and implement them at work. Here is a Proof-of-Concept example.
Recommendation system is about recommending items to users. Let&amp;rsquo;s say we only have:
 Items: 6 articles: [&amp;lsquo;dog1&amp;rsquo;, &amp;lsquo;dog2&amp;rsquo;, &amp;lsquo;cat&amp;rsquo;, &amp;lsquo;food1&amp;rsquo;, &amp;lsquo;food2&amp;rsquo;, &amp;lsquo;food3&amp;rsquo;]. (Note: we label them just for easier explanation, it is not necessary to have any context about the items, they can be random ids) Users: 5 visitors [&amp;lsquo;user_0&amp;rsquo;,&amp;lsquo;user_1&amp;rsquo;,&amp;lsquo;user_2&amp;rsquo;, &amp;lsquo;user_3&amp;rsquo;, &amp;lsquo;user_4&amp;rsquo;, &amp;lsquo;user_5&amp;rsquo;]  Collaborative Filtering  Collaborative filteringÂ approaches build a model from a user&amp;rsquo;s past behaviour (items previously purchased or selected and/or numerical ratings given to those items) as well as similar decisions made by other users.</description>
    </item>
    
    <item>
      <title>How does imputation work</title>
      <link>https://darwinleung.github.io/posts/2018-05-12-imputation-sklearn/</link>
      <pubDate>Sat, 12 May 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-05-12-imputation-sklearn/</guid>
      <description>How does imputation work? Imputation is a pre-processing technique to handle missing data before fitting the data into model. One of the simplest implementation is to estimate the missing values using the mean/median or the most frequent value of the row/ column where the missing values are located. There are more advanced ways to impute data like regressions, multiple imputation etc.
Imputer in sklearn The Imputer function in the sklearn library is commonly used in various tutorials.</description>
    </item>
    
    <item>
      <title>Extracting features from text - BoW, tfidf</title>
      <link>https://darwinleung.github.io/posts/2018-05-09-extracting-features-from-text-bow-tfidf/</link>
      <pubDate>Wed, 09 May 2018 12:00:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-05-09-extracting-features-from-text-bow-tfidf/</guid>
      <description>Most of the machine learning algorithms work with numerical features only, they cannot work with raw text directly. Hence, we need to extract/generate numerical representation of textual data. One of the simplest way is to use the occurance or frequency of words.
Bag-of-words (BoW)  Representation of text that describes the occurence of words within a document
 Step:
 Find all unique words in a document, used as column (aka token) Define the measure method (1&amp;frasl;0 for presence or absence of words, count, tfidf) Calculate the measurement for each sentence, each row correspond to each sentence (aka tokenization)   Example:</description>
    </item>
    
    <item>
      <title>Running python 2 and 3 using conda</title>
      <link>https://darwinleung.github.io/posts/2018-03-30-running-python-2-and-3-using-conda/</link>
      <pubDate>Fri, 30 Mar 2018 12:37:00 -0400</pubDate>
      
      <guid>https://darwinleung.github.io/posts/2018-03-30-running-python-2-and-3-using-conda/</guid>
      <description>I am taking the M101P course to brush up my Mongo skill, the course is kind of dated, it uses Python 2. However, I use Python 3 on my machine by default. I need a quick and easy way to run python 2 scripts without changing my default settings/path. I found conda offer a great out-of-box solution by using environment.
 A conda environment is a directory that contains a specific collection of conda packages that you have installed.</description>
    </item>
    
  </channel>
</rss>